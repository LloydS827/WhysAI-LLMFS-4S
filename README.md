![240624_llm_roadmap.drawio](./assets/240624_llm_roadmap.drawio.png)

### Description

WhysAI--**LLMFS**项目是由一系列mini-projects组成、围绕大模型主题的系列课程。主要包括三个系列：

1. [CB](https://github.com/LloydS827/WhysAI-LLMFS-CB)：深度学习基础，包括神经网络、反向传播、PyTorch等；
2. [4S(This Repo)](https://github.com/LloydS827/WhysAI-LLMFS-4S)：以从零到一训练/微调大语言模型为核心；
3. [4E](https://github.com/LloydS827/WhysAI-LLMFS-4E)：以LLM应用为核心，涵盖RAG、Agent、MAS、Inference等。

本项目受到许多优秀的开源课程与资料启发，制作的课程资料包括Slides、Code、Video等，既用来记录学习现代人工智能技术的过程，又希望与大家共同探索AI技术的最佳学习路径。

### 4S(For Scientist)系列课程内容（V0.1, 持续更新中）
| Theme & Contents          | Course Code & Materials | Project & Keywords | Videos | References                                                   |
| ------------------------- | ----------------------- | ------------------ | ------ | ------------------------------------------------------------ |
| RNN/LSTM /GRUs            |                         |                    |        | 1. [Recurrent Neural Networks (RNNs), Clearly Explained!!! - YouTube](https://www.youtube.com/watch?v=AsNTP8Kwu80)<br>2. [The Unreasonable Effectiveness of Recurrent Neural Networks (karpathy.github.io)](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)<br>3.[Long Short-Term Memory (LSTM), Clearly Explained - YouTube](https://www.youtube.com/watch?v=YCzL96nL7j0) |
|                           |                         |                    |        |                                                              |
| Embedding/Tokenizer       |                         |                    |        | 1. [Tiktokenizer](https://tiktokenizer.vercel.app/)<br>2. [Let's build the GPT Tokenizer - YouTube](https://www.youtube.com/watch?v=zduSFxRajkE&t=8s)<br> |
| Transformer/Decoder-only  |                         |                    |        | <br>- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar: A visual and intuitive explanation of the Transformer model.<br>- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) by Jay Alammar: Even more important than the previous article, it is focused on the GPT architecture, which is very similar to Llama's.<br>- [Visual intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M&t=187s) by 3Blue1Brown: Simple easy to understand visual intro to Transformers<br>- [LLM Visualization](https://bbycroft.net/llm) by Brendan Bycroft: Incredible 3D visualization of what happens inside of an LLM.<br>- [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy: A 2h-long YouTube video to reimplement GPT from scratch (for programmers).<br>- [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) by Lilian Weng: Introduce the need for attention in a more formal way.<br>- [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html): Provide code and a visual introduction to the different decoding strategies to generate text.<br> |
| PT                        |                         |                    |        | 1. [Embedding Models: From Architecture to Implementation - DeepLearning.AI](https://www.deeplearning.ai/short-courses/embedding-models-from-architecture-to-implementation/)<br>2.[Pretraining LLMs - DeepLearning.AI](https://www.deeplearning.ai/short-courses/pretraining-llms/)<br>3.  [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)<br>4. [Let's reproduce GPT-2 (124M) - YouTube](https://www.youtube.com/watch?v=l8pRSuU81PU)<br>5.[Let's build GPT: from scratch, in code, spelled out. - YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7)6. [Let's build the GPT Tokenizer - YouTube](https://www.youtube.com/watch?v=zduSFxRajkE&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=9) |
| FT                        |                         |                    |        | 1. [Federated Learning - DeepLearning.AI](https://www.deeplearning.ai/short-courses/intro-to-federated-learning/)<br>2. [Finetuning Large Language Models - DeepLearning.AI](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)<br> |
| . Transformer Improvement |                         |                    |        |                                                              |


### Key Resources

> 每期参考/推荐资料见“课程内容”

**Key Roadmap & Resources:**

- [GenAI Handbook (genai-handbook.github.io)](https://genai-handbook.github.io/)
- [https://github.com/mlabonne/llm-course](https://github.com/mlabonne/llm-course)
- [karpathy/LLM101n: LLM101n: Let's build a Storyteller (github.com)](https://github.com/karpathy/LLM101n)

**Important：**
- [StatQuest](https://space.bilibili.com/3546620985608836?spm_id_from=333.337.0.0)
- Deeplearning.ai
- Andrej Karparthy: Zero 2 Hero Series
- 3B1B: NN series
- VisualizeML(Iris Series)

**Others**：

- AI Infrastructure [https://github.com/chenzomi12/AISystem/](https://github.com/chenzomi12/AISystem/)
- ML-data engineering [https://github.com/GokuMohandas/Made-With-ML](https://github.com/GokuMohandas/Made-With-ML)
- NN-deeplearning [https://zh.d2l.ai/](https://zh.d2l.ai/)
- Math & Python & ML [https://github.com/Visualize-ML](https://github.com/Visualize-ML)
- AI Papers w/ code [https://github.com/labmlai/annotated_deep_learning_paper_implementations](https://github.com/labmlai/annotated_deep_learning_paper_implementations
- Rag From Stratch by langchain: [https://github.com/langchain-ai/rag-from-scratch](https://github.com/langchain-ai/rag-from-scratch)
- Qwen Docs: [https://qwen.readthedocs.io/zh-cn/latest/index.html#](https://qwen.readthedocs.io/zh-cn/latest/index.html#)
- the bitter lesson: [https://cloud.tencent.com/developer/article/2119875](https://cloud.tencent.com/developer/article/2119875)
- [datawhalechina/llm-cookbook: 面向开发者的 LLM 入门教程，吴恩达大模型系列课程中文版 (github.com)](https://github.com/datawhalechina/llm-cookbook)
- langchain master class: [hw_dungeonrooms4q_h_en_115 (youtube.com)](https://www.youtube.com/watch?v=yF9kGESAi3M)

**Fundalmentals**

1. Python: [30 days of Python](https://github.com/Asabeneh/30-Days-Of-Python)
2. Math & Machine Learning:[Visualize-ML (Iris Series)](https://github.com/Visualize-ML)
3. Deep Learning: [D2l](https://courses.d2l.ai/zh-v2/)

### WhysAI

WhysAI由复旦大学几位在读博士发起，希望通过持续探索，来迈向卓越。Learn all we can，do the experiments，embrace the failures，then succeed。同时，欢迎关注公众号“WhysAI怀思智合”。

